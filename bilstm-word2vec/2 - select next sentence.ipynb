{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation using Bidirectional LSTM and Doc2Vec models 2/3\n",
    "\n",
    "If you have reached directly this page, I suggest to start reading the first part of this article. It describes how to create a RNN model to generate a text, word after word.\n",
    "\n",
    "I finished the first part of the article explaining I will try to improve the generation of sentences, by detecting patterns in the sequences of sentences, not only in the sequences of words.\n",
    "\n",
    "It could be an improvement, because doing that, the context of a paragraph (is it a description of a countryside? a dialog between characters? which people are involved? what are the previous actions? etc.) could emerge and can be used to select wisely the next sentence of the text.\n",
    "\n",
    "The process will be similar to the previous one, however, I will have to vectorize all sentences in the text, and try to find patterns in sequences of these vectors.\n",
    "\n",
    "In order to do that, we will use **Doc2Vec**.\n",
    "\n",
    "# 1. Doc2Vec\n",
    "Doc2Vec is able to vectorize a paragraph of text. If you do not know it, I suggest to have a look on the gensim web site, that describes how its work and what you’re allowed to do with it.\n",
    "\n",
    "In a short, we will transform each sentences of our text in a vector of a specific space. The great thing of the approach is we will be able to compare them ; by example, to retrieve the most similar sentence of a given one.\n",
    "\n",
    "Last but not least, the dimension of the vectors will be the same, whatever is the number of words in their linked sentence.\n",
    "\n",
    "It is exactly what we are looking for: I will be able to train a new LSTM, trying to catch pattern from sequences of vectors of the same dimensions.\n",
    "\n",
    "``I have to be honest: I am not sure we can perform such task with enough accuracy, but let’s have some tests. It is an experiment, at worst, it will be a good exercice.``\n",
    "\n",
    "So, once all sentences will be converted to vectors, we will try to **train a new bidirectional LSTM**. It purpose will be to predict the best vector, next to a sequence of vectors.\n",
    "\n",
    "Then how will we generate text ?\n",
    "\n",
    "Pretty easy: thanks to our previous LSTM model, we will generate sentences as candidates to be the next phrase. We will infer their vectors using the **trained doc2Vec model**, then pick the closest one to the prediction of our new LSTM model.\n",
    "\n",
    "## 1.1 Create the Doc2Vec Model\n",
    "The first task is to create our **doc2vec model**, dedicated to our text and embedded sentences.\n",
    "\n",
    "**Doc2Vec** assumes its input to be a list a words, with a label, per sentence:\n",
    "\n",
    "``Example: ['tobus', 'ouvre', 'la', 'porte', '.'] LABEL1``\n",
    "\n",
    "So we have to extract from the text each sentences and splits their words.\n",
    "\n",
    "by convention, I assume a sentence ends with “.”,”?”,”!”,”:” or “…”. The script reads each text, and create a new sentence each time it reaches on of these characters.\n",
    "\n",
    "First, we load the Doc2Vec library, we load our data and set some parameter:\n",
    "\n",
    "- all texts are stored in the **data_dir** directory,\n",
    "- the **file_list** list contains the names of all text files in the **data_dir** directory,\n",
    "- the **save_dir** will be used to save models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim library\n",
    "import gensim\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from __future__ import print_function\n",
    "import double_log\n",
    "def print(*args, **kwargs):\n",
    "    return double_log.print(*args, **kwargs)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "\n",
    "#parameters\n",
    "data_dir = 'data/essays'# data directory containing input.txt\n",
    "save_dir = 'save' # directory to store models\n",
    "file_list = ['s11.txt', '21.txt', 'y_ad1.txt', '19.txt', 'b2.txt', 'me43.txt', 'g8.txt', 'me10.txt', 'r8.txt', 'b3.txt', 'b11.txt', '29.txt', 'b14.txt', 'n3.txt', 's3.txt', 'me26.txt', 'a1.txt', '13.txt', 'b13.txt', 'z.txt', '16.txt', 'me11.txt', 's10.txt', 'me8.txt', 'me30.txt', 'me35.txt', 'me20.txt', 'me42.txt', 'a12.txt', 'me28.txt', 'me22.txt', 'y_ad5.txt', 'd1.txt', '24.txt', 'r3.txt', 'y_ad7.txt', 'd10.txt', 'me6.txt', 'me41.txt', 'me39.txt', 'n1.txt', 'imon.txt', 'me7.txt', 'a8.txt', 'me36.txt', 'me44.txt', 'a11.txt', 'g4.txt', 'me38.txt', 'd8.txt', 'a9.txt', 'me9.txt', 'me2.txt', '20.txt', '28.txt', 'me46.txt', 'b7.txt', 'me31.txt', 'g9.txt', 'y_ad4.txt', 'r5.txt', 'b1.txt', 'r7.txt', '26.txt', 'n11.txt', 's9.txt', 's4.txt', 'me40.txt', 'r1.txt', 'me24.txt', 'me29.txt', 'me14.txt', 's12.txt', 'me17.txt', 'me12.txt', 'n7.txt', 's6.txt', 'b12.txt', 'b16.txt', 'one.txt', 'b9.txt', 'a4.txt', 'r2.txt', 'r12.txt', 'd2.txt', 'b15.txt', 'r6.txt', 'me1.txt', 'd9.txt', 'd6.txt', 'g2.txt', 'me34.txt', 'b8.txt', 'r4.txt', 'four.txt', 'me32.txt', 'n12.txt', 'me19.txt', 'g1.txt', 's8.txt']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create the list of sentences for the doc2vec model: to split easily sentences, I use the **spaCy** library. Then, I create the a list of Labels for these sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyuad/anaconda2/envs/Gabor/lib/python2.7/site-packages/ipykernel_launcher.py:34: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/home/nyuad/anaconda2/envs/Gabor/lib/python2.7/site-packages/ipykernel_launcher.py:35: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#import spacy, and french model\n",
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re, unicodedata\n",
    "from sets import Set\n",
    "from string import maketrans\n",
    "\n",
    "def norm_text(text):\n",
    "    table = {768: u\"'\", 769: u\"'\", 770: u' ', 771: u' ', 772: u'-', 774: u' ', 4359: u' ', 776: u'\"', 778: u' ', 8203: u' ', 8208: u'-', 8211: u'-', 8212: u'-', 19990: u' ', 8216: u\"'\", 8217: u\"'\", 8730: u' ', 8220: u'\"', 8221: u'\"', 8222: u'\"', 25379: u' ', 164: u' ', 807: u' ', 8232: u' ', 8364: u' ', 173: u' ', 4527: u' ', 176: u' ', 38065: u' ', 946: u' ', 949: u' ', 779: u'\"', 8260: u' ', 198: u' ', 4363: u' ', 4449: u' ', 9702: u' ', 34945: u' ', 295: u' ', 20975: u' ', 247: u' ', 65279: u' '}\n",
    "    text = re.sub(' +', ' ', data)\n",
    "    text = re.sub('[\\r\\n]+', '\\n', text)\n",
    "    text = unicodedata.normalize('NFKD', unicode(text, \"utf-8\"))\n",
    "    text = text.translate(table)\n",
    "    return text\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "#initiate sentences and labels lists\n",
    "sentences = []\n",
    "sentences_label = []\n",
    "\n",
    "#create sentences function:\n",
    "# def create_sentences(doc):\n",
    "#     #print(doc)\n",
    "#     return sent_tokenize(doc)\n",
    "\n",
    "#create sentences function:\n",
    "def create_sentences(doc):\n",
    "    ponctuation = [\".\",\"?\",\"!\",\":\",\"…\"]\n",
    "    sentences = []\n",
    "    sent = []\n",
    "    for word in doc:\n",
    "        if word.text not in ponctuation:\n",
    "            if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n",
    "                sent.append(word.text.lower())\n",
    "        else:\n",
    "            sent.append(word.text.lower())\n",
    "            if len(sent) > 1:\n",
    "                sentences.append(sent)\n",
    "            sent=[]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "#create sentences from files\n",
    "for file_name in file_list:\n",
    "    input_file = os.path.join(data_dir, file_name)\n",
    "    #read data\n",
    "    with codecs.open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "    data = norm_text(data)\n",
    "    #create sentences\n",
    "    doc = nlp(data)\n",
    "    sents = create_sentences(doc)\n",
    "    sentences = sentences + sents\n",
    "    \n",
    "#create labels\n",
    "for i in range(np.array(sentences).shape[0]):\n",
    "    sentences_label.append(\"ID\" + str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Train doc2vec model\n",
    "As explained above, **doc2vec** required its inputs to be correctly shaped. In order to do that, we define a specific class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield gensim.models.doc2vec.LabeledSentence(doc,[self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also create a specific function to train the doc2vec model. Its purpose is to update easily training paramaters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec_model(data, docLabels, size=300, sample=0.000001, dm=0, hs=1, window=10, min_count=0, workers=8,alpha=0.024, min_alpha=0.024, epoch=15, save_file='./data/doc2vec.w2v') :\n",
    "    startime = time.time()\n",
    "    \n",
    "    print(\"{0} articles loaded for model\".format(len(data)))\n",
    "\n",
    "    it = LabeledLineSentence(data, docLabels)\n",
    "\n",
    "    model = gensim.models.Doc2Vec(size=size, sample=sample, dm=dm, window=window, min_count=min_count, workers=workers,alpha=alpha, min_alpha=min_alpha, hs=hs) # use fixed learning rate\n",
    "    model.build_vocab(it)\n",
    "    for epoch in range(epoch):\n",
    "        print(\"Training epoch {}\".format(epoch + 1))\n",
    "        model.train(it,total_examples=model.corpus_count,epochs=model.iter)\n",
    "        # model.alpha -= 0.002 # decrease the learning rate\n",
    "        # model.min_alpha = model.alpha # fix the learning rate, no decay\n",
    "        \n",
    "    #saving the created model\n",
    "    model.save(os.path.join(save_file))\n",
    "    print('model saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "few notes regarding the parameters of the function: the default parameters have been chosen empirically.\n",
    "\n",
    "Now, it's time to train the **doc2vec model**. Simply run the command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7383 articles loaded for model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyuad/anaconda2/envs/Gabor/lib/python2.7/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "/home/nyuad/anaconda2/envs/Gabor/lib/python2.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyuad/anaconda2/envs/Gabor/lib/python2.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 2\n",
      "Training epoch 3\n",
      "Training epoch 4\n",
      "Training epoch 5\n",
      "Training epoch 6\n",
      "Training epoch 7\n",
      "Training epoch 8\n",
      "Training epoch 9\n",
      "Training epoch 10\n",
      "Training epoch 11\n",
      "Training epoch 12\n",
      "Training epoch 13\n",
      "Training epoch 14\n",
      "Training epoch 15\n",
      "Training epoch 16\n",
      "Training epoch 17\n",
      "Training epoch 18\n",
      "Training epoch 19\n",
      "Training epoch 20\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "train_doc2vec_model(sentences, sentences_label, size=500,sample=0.0,alpha=0.025, min_alpha=0.001, min_count=0, window=10, epoch=20, dm=0, hs=1, save_file='./data/doc2vec.w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some insights for the used parameters:\n",
    "\n",
    "- **dimensions**: 300 dimensions seem to work well for classic subjects. In my case, after few tests, I prefer to choose 500 dimensions,\n",
    "- **epochs**: below 10 epochs, results are not good enough (similiary are not working well), and bigger number of epochs creates to much similar vectors. So I chose 20 epochs for the training.\n",
    "- **min_count**: I want to integrate all words in the training, even those with very few occurence. Indeed, I assume that, for my exercice, specific words could be important. I set the value to 0, but 3 to 5 should be OK.\n",
    "- **sample**: *0.0*. I do not want to downsample randomly higher-frequency words, so I disabled it.\n",
    "- **hs and dm**: Each time I want to infer a new vector from the trained model, for a given sentence, I want to have the same output vector. In order to do that (strangly it’s not so intuitive), I need to use a distributed bag of words as *training algorithm (dm=0)* and *hierarchical softmax (hs=1)*. Indeed, for my purpose, distributive memory and negative sampling seems to give less good results.\n",
    "\n",
    "# 2. Create the Input Dataset\n",
    "First, using my trained **doc2Vec**, I will infer the vector for all sentences of my texts. The **doc2vec** model will provide directly the vector of each sentence, we just have to iterate over the whole sentences list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 0 : [u'are', u'charter', u'cities', u'desirable', u'or', u'feasible', u'?']\n",
      "***\n",
      "sentence 500 : [u'even', u'if', u'there', u'is', u'only', u'some', u'isolated', u'economic', u'activities', u'that', u'could', u'lead', u'to', u'perceive', u'the', u'incas', u'as', u'socialist', u',', u'why', u'would', u'such', u'idea', u'even', u'arise', u'in', u'academia', u'?']\n",
      "***\n",
      "sentence 1000 : [u'patsy', u'chooses', u'which', u'episodes', u'from', u'her', u'childhood', u'are', u'worth', u'remembering', u'and', u'how', u'to', u'portray', u'them', u'.']\n",
      "***\n",
      "sentence 1500 : [u'gideon', u'answers', u'the', u'phone', u':']\n",
      "***\n",
      "sentence 2000 : [u'it', u'is', u'by', u'rejecting', u'all', u'that', u'make', u'us', u'cling', u'to', u'the', u'barbaric', u'\"', u'civilization', u'\"', u'.']\n",
      "***\n",
      "sentence 2500 : [u'difference', u'between', u'visual', u'and', u'the', u'image', u'in', u'one', u'of', u'his', u'last', u'columns', u'to', u'the', u\"libe'ration\", u',', u'serge', u'daney', u'wrote', u'about', u'the', u'relationships', u'between', u'the', u'gulf', u'war', u',', u'video', u'journalism', u',', u'and', u'the', u'role', u'of', u'image', u',', u'in', u'reaction', u'to', u'the', u'lack', u'of', u'proper', u'video', u'journalism', u'displayed', u'on', u'french', u'television', u'during', u'the', u'gulf', u'war', u'.']\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "from six.moves import cPickle\n",
    "\n",
    "#load the model\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec.load('./data/doc2vec.w2v')\n",
    "\n",
    "sentences_vector=[]\n",
    "\n",
    "t = 500\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    if i % t == 0:\n",
    "        print(\"sentence\", i, \":\", sentences[i])\n",
    "        print(\"***\")\n",
    "    sent = sentences[i]\n",
    "    sentences_vector.append(d2v_model.infer_vector(sent, alpha=0.001, min_alpha=0.001, steps=10000))\n",
    "    \n",
    "#save the sentences_vector\n",
    "sentences_vector_file = os.path.join(save_dir, \"sentences_vector_500_a001_ma001_s10000.pkl\")\n",
    "with open(os.path.join(sentences_vector_file), 'wb') as f:\n",
    "    cPickle.dump((sentences_vector), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I do not use vectors generated during the training, because I want to compare them to vectors infered for sentences it did not seen. It’s better to generate them in the same way.\n",
    "\n",
    "Now, in order to create the Keras input data set **(X_train, y_train)**, we have to folow these guidelines:\n",
    "\n",
    "- 15 sequenced vectors from doc2vec as input,\n",
    "- the next vector (16th) as output.\n",
    "\n",
    "so, the dimension of X_train must be **(number of sequences, 15, 500)** and the dimension of y_train: **(number of sequences, 500)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_sequenced_sentences = 15\n",
    "vector_dim = 500\n",
    "\n",
    "X_train = np.zeros((len(sentences), nb_sequenced_sentences, vector_dim), dtype=np.float)\n",
    "y_train = np.zeros((len(sentences), vector_dim), dtype=np.float)\n",
    "\n",
    "t = 1000\n",
    "for i in range(len(sentences_label)-nb_sequenced_sentences-1):\n",
    "    if i % t == 0: print(\"new sequence: \", i)\n",
    "    \n",
    "    for k in range(nb_sequenced_sentences):\n",
    "        sent = sentences_label[i+k]\n",
    "        vect = sentences_vector[i+k]\n",
    "        \n",
    "        if i % t == 0:\n",
    "            print(\"  \", k + 1 ,\"th vector for this sequence. Sentence \", sent, \"(vector dim = \", len(vect), \")\")\n",
    "            \n",
    "        for j in range(len(vect)):\n",
    "            X_train[i, k, j] = vect[j]\n",
    "    \n",
    "    senty = sentences_label[i+nb_sequenced_sentences]\n",
    "    vecty = sentences_vector[i+nb_sequenced_sentences]\n",
    "    if i % t == 0: print(\"  y vector for this sequence \", senty, \": (vector dim = \", len(vecty), \")\")\n",
    "    for j in range(len(vecty)):\n",
    "        y_train[i, j] = vecty[j]\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create the Keras Model\n",
    "\n",
    "Great, let's create the model now…\n",
    "\n",
    "First, we load the library and create the function to define a simple keras Model:\n",
    "\n",
    "- bidirectional LSTM,\n",
    "- with size of 512 and using RELU as activation (very small, but quicker to perform the test),\n",
    "- then a dropout layer of 0,5.\n",
    "- The network will not provide me a probability but directly the next vector for a given sequence. So I finish it with a simple dense layer of the size of the vector dimension.\n",
    "\n",
    "I use ADAM as optimizer and the loss calculation is done using **logcosh**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, Flatten, Bidirectional, Input, LSTM\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_accuracy, mean_squared_error, mean_absolute_error, logcosh\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "def bidirectional_lstm_model(seq_length, vector_dim):\n",
    "    print('Building LSTM model...')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\"),input_shape=(seq_length, vector_dim)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(vector_dim))\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    callbacks=[EarlyStopping(patience=2, monitor='val_loss')]\n",
    "    model.compile(loss='logcosh', optimizer=optimizer, metrics=['acc'])\n",
    "    print('LSTM model built.')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_size = 512 # size of RNN\n",
    "vector_dim = 500\n",
    "learning_rate = 0.0001 #learning rate\n",
    "\n",
    "model_sequence = bidirectional_lstm_model(nb_sequenced_sentences, vector_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30 # minibatch size\n",
    "\n",
    "callbacks=[EarlyStopping(patience=3, monitor='val_loss'),\n",
    "           ModelCheckpoint(filepath=save_dir + \"/\" + 'my_model_sequence_lstm.{epoch:02d}.hdf5',\\\n",
    "                           monitor='val_loss', verbose=1, mode='auto', period=5)]\n",
    "\n",
    "history = model_sequence.fit(X_train, y_train,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True,\n",
    "                 epochs=40,\n",
    "                 callbacks=callbacks,\n",
    "                 validation_split=0.1)\n",
    "\n",
    "#save the model\n",
    "model_sequence.save(save_dir + \"/\" + 'my_model_sequence_lstm.final2.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great ! After few hours of training, we have trained a model to predict the next best sentence vector for a given sequence of sentences.\n",
    "\n",
    "Few remarks regarding the results:\n",
    "\n",
    "the loss drop to 0.1049, the accuracy is around 14%,\n",
    "the val_loss is around 0.1064 with val accuracy around 16%.\n",
    "\n",
    "# 4. Conclusion\n",
    "As you probably noticed, the raw result of the neural networks trained during the tutorial is not \"amazing\"… We can probably do better.\n",
    "\n",
    "However, let's check if the exercice is good enough to select the best next sentence of a text. I hope it will be fair enough for my test, indeed, for a given sequence of sentences, there is no clear determinism in the sequence to be chosen.\n",
    "\n",
    "In order to test that, we have to, for a given sequence of sentences:\n",
    "\n",
    "- generate, using our **first LSTM model**, different candidates of sentences,\n",
    "- Infer their vectors using our **doc2vec model**,\n",
    "- Generate, using our **second LSTM model**, the best following vector for the sequence,\n",
    "- then select the most similar vector.\n",
    "- That’s what I’ll try to do in the next part of this experiment…\n",
    "\n",
    "Thanks for reading !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
